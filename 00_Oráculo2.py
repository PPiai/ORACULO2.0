import streamlit as st
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import requests
from bs4 import BeautifulSoup

# Configura칞칚o do pipeline do modelo Hugging Face
@st.cache_resource
def carregar_pipeline():
    modelo = "gpt2"  # Substitua por "flan-t5-base" para respostas mais espec칤ficas
    tokenizer = AutoTokenizer.from_pretrained(modelo)
    modelo_transformers = AutoModelForCausalLM.from_pretrained(modelo)
    return pipeline("text-generation", model=modelo_transformers, tokenizer=tokenizer, max_length=500)

TIPOS_ARQUIVOS_VALIDOS = [
    'Tech',
    'Social Media',
    'Gestor de Tr치fego',
    'Account',
    'Vendas'
]

ARQUIVOS = {
    'Tech': ['https://vendas.v4company.com/glossario-marketing/'],
    'Social Media': ['https://vendas.v4company.com/glossario-marketing/'],
    'Gestor de Tr치fego': ['https://vendas.v4company.com/glossario-marketing/'],
    'Account': ['https://vendas.v4company.com/glossario-marketing/'],
    'Vendas': ['https://vendas.v4company.com/glossario-marketing/'],
}

# Fun칞칚o para carregar conte칰do das URLs e extrair texto
def carrega_site(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            return soup.get_text(separator=" ", strip=True)
        else:
            return f"Erro ao carregar o site: {response.status_code}"
    except Exception as e:
        return f"Erro ao conectar: {e}"

# Fun칞칚o para criar uma resposta din칙mica baseada no modelo Hugging Face
def criar_resposta(pergunta, tipo_arquivo, pipeline_ia):
    # Carregar os documentos relacionados ao tipo de arquivo selecionado
    arquivos = ARQUIVOS.get(tipo_arquivo, [])
    contexto = ""
    for url in arquivos:
        contexto += carrega_site(url) + "\n\n"

    # Criar o prompt para o modelo
    prompt = f"Contexto: {contexto}\n\nPergunta: {pergunta}\n\nResposta:"
    
    # Gerar resposta usando o pipeline Hugging Face
    resposta = pipeline_ia(prompt)[0]["generated_text"]
    return resposta

# Exibir mensagens no chat
def pagina_chat(pipeline_ia):
    st.title("游뱄 Bem-vindo ao Or치culo")
    st.divider()

    # Inicializar hist칩rico de mensagens
    if "historico" not in st.session_state:
        st.session_state["historico"] = []

    # Exibir hist칩rico de mensagens
    for mensagem in st.session_state["historico"]:
        st.chat_message(mensagem["autor"]).markdown(mensagem["conteudo"])

    # Entrada do usu치rio
    pergunta = st.chat_input("Fale com o Or치culo")
    if pergunta:
        # Registrar pergunta do usu치rio
        st.session_state["historico"].append({"autor": "human", "conteudo": pergunta})
        st.chat_message("human").markdown(pergunta)

        # Gerar resposta
        tipo_arquivo = st.session_state.get("tipo_arquivo", "Tech")
        resposta = criar_resposta(pergunta, tipo_arquivo, pipeline_ia)

        # Construir resposta final
        st.session_state["historico"].append({"autor": "ai", "conteudo": resposta})
        st.chat_message("ai").markdown(resposta)

# Configura칞칚o da barra lateral
def sidebar():
    st.sidebar.title("Configura칞칚o")
    
    # Sele칞칚o da base de conhecimento
    tipo_arquivo = st.sidebar.selectbox("Selecione sua 츼rea", TIPOS_ARQUIVOS_VALIDOS)
    st.session_state["tipo_arquivo"] = tipo_arquivo

    # Bot칚o para apagar hist칩rico
    if st.sidebar.button("Apagar Hist칩rico"):
        st.session_state["historico"] = []

# Executar aplicativo principal
def main():
    pipeline_ia = carregar_pipeline()
    sidebar()
    pagina_chat(pipeline_ia)

if __name__ == "__main__":
    main()
